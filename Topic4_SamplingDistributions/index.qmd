---
title: "Topic 4 - The Normal Distribution and Sampling Variability"
format: 
  revealjs:
    theme: OSU.scss
    chalkboard: false
    html-math-method: mathjax
    multiplex: true
editor: visual
---

## Download or print notes to PDF {.smaller}

If you'd like to export this presentation to a PDF, do the following

::: nonincremental
1.  Toggle into Print View using the E key.
2.  Open the in-browser print dialog (CTRL/CMD+P)
3.  Change the **Destination** to **Save as PDF**.
4.  Change the **Layout** to **Landscape**.
5.  Change the **Margins** to **None**.
6.  Enable the **Background graphics** option.
7.  Click **Save**.
:::

This feature has been confirmed to work in Google Chrome and Firefox.

# The Normal Distribution

## Let's start with an example {.smaller}

In a study on driving reaction time, researchers record the time (in seconds) that it takes study participants to react to the brake lights of a vehicle traveling in front of them.

A histogram of the recorded data is shown below.

```{r}
#| fig-align: "center"
library(latex2exp)
library(tidyverse)
library(viridis)

set.seed(8355)
react <- rnorm(505, 1.25, 0.46)
react_df <- data.frame(react)
react_df <- react_df |>
  filter(react > 0)
h <- ggplot(react_df, aes(x = react)) + 
  geom_histogram(color = "black", fill = "#D73F09", 
                 breaks = seq(0, max(react_df$react), 
                              length.out = 15)) + 
  labs(x = "Reaction Time (s)", y = "Frequency") + 
  theme_bw()
h
```

::: fragment
Describe the shape, center, and spread of the distribution.

```{r}
library(countdown)
countdown(minutes = 1)
```
:::

## Normal Distribution {.smaller}

:::::::: columns
:::: column
-   When to use:

[Useful when modeling a continuous random variable that has a bell-shaped distribution.]{.fragment style="color:blue;"}

::: {.fragment style="color:blue;"}
```{r}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = viridis(6)[3], linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() + 
  scale_x_continuous(breaks = NULL)
```
:::
::::

::::: column
-   Parameters of the distribution:

::: {.fragment style="color:blue;"}
$\mu$: mean - determines the center of the distribution

$\sigma$: standard deviation - determines the spread of the distribution
:::

-   Probability Density Function:

::: {.fragment style="color:blue;"}
$$f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ for $x$ in $(-\infty, \infty)$
:::

Â 

-   Expectation: [$E(X) = \mu$]{.fragment style="color:blue;"}

-   Variance:[$Var(X) = \sigma^2$]{.fragment style="color:blue;"}
:::::
::::::::

## Example: Driving Reaction Time

```{r}
h_d <- ggplot(react_df, aes(x = react)) + 
  geom_histogram(aes(y=..density..), 
                 color = "black", fill = "#D73F09", 
                 breaks = seq(0, max(react_df$react), 
                              length.out = 15)) + 
  labs(x = "Reaction Time (s)", y = "Density") + 
  theme_bw() + 
  theme(axis.title.y = element_text(size = 18)) + 
  stat_function(fun = dnorm, n = 101, args = list(mean = 1.25, sd = 0.46), color = viridis(6)[3], linewidth = 2)

h_d
```

## Standard Normal Distribution

[The *Standard* Normal Distribution is a Normal distribution with mean $\mu=0$ and standard deviation $\sigma=1$.]{.fragment style="color:blue;"}

::: {.fragment style="color:blue;"}
$$Z \sim N(0, 1)$$
:::

::: fragment
```{r}
#| fig-height: 4
library(latex2exp)
library(tidyverse)
library(viridis)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = viridis(6)[3], linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() + 
  scale_x_continuous(breaks = seq(-3,3,1))
```
:::

## Standardizing Any Normal Random Variable {.smaller}

[For a Normal random variable, $X$, a z-score represents the number of standard deviations any observation $x$ is from the mean.]{.fragment style="color:blue;"}

::: {.fragment style="color:blue;"}
$$z = \frac{x-\mu}{\sigma}$$
:::

::::::: columns
:::: {.column width="50%"}
::: fragment
```{r}
#| fig-height: 6
library(latex2exp)
library(tidyverse)
library(viridis)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = viridis(6)[1], linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() + 
    theme(axis.text.x = element_text(size = 18, color = viridis(6)[1], face = "bold")) + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = seq(-2,2,1), 
                     labels = c(TeX(r'(\mu - 2\sigma)'),
                                TeX(r'(\mu - \sigma)'), 
                                TeX(r'(\mu)'), 
                                TeX(r'(\mu + \sigma)'), 
                                TeX(r'(\mu + 2\sigma)')))
```
:::
::::

:::: {.column width="50%"}
::: {.fragment style="color:blue;"}
```{r}
#| fig-height: 6
library(latex2exp)
library(tidyverse)
library(viridis)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = viridis(6)[3], linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() + 
    theme(axis.text.x = element_text(size = 18, color = viridis(6)[3], face = "bold")) + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = seq(-2,2,1))
```
:::
::::
:::::::

## Example: Driving Reaction Time {.smaller}

The driving reaction time study suggests that reaction times to break lights are Normally distributed with a mean of 1.25 seconds and standard deviation of 0.46.

Consider a driver who reacted in 0.79 seconds. How many standard deviations away from the mean is this driver's reaction time?

::: fragment
[Calculate the z-score!]{style="color:blue;"}

```{r}
countdown(minutes = 0, seconds = 45)
```
:::

[$z=$$\frac{x-\mu}{\sigma}$$=\frac{0.79-1.25}{0.46}$$=-1$]{.fragment style="color:blue;"}

Â 

[The driver's reaction time is 1 *standard deviation* less than (i.e., faster than) the average driver in the study.]{.fragment style="color:blue;"}

## Example: Driving Reaction Time {.smaller}

The driving reaction time study suggests that reaction times to break lights are Normally distributed with a mean of 1.25 seconds and standard deviation of 0.46.

What is the probability of a randomly selecting a driver with a reaction time less than 1 second?

:::::: columns
:::: column
::: {.fragment style="color:blue;"}
```{r}
#| fig-height: 6

ggplot(data = data.frame(x = c(0,2.5)), aes(x)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = 1.25, sd = 0.46), color = viridis(6)[3], linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() + 
    theme(axis.text.x = element_text(size = 18, color = viridis(6)[3], face = "bold")) + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = c(0.33, 0.79, 1.25, 1.71, 2.17))
```
:::
::::

::: column
[$$P(X < 1) = \int \limits_{-\infty}^{1} \frac{1}{\sqrt{2\pi (0.46^2)}}e^{-\frac{(x-1.25)^2}{2(0.46^2)}} dx$$]{.fragment style="color:blue;"}

Â 

[We cannot solve this analytically - we must use R!]{.fragment style="color:blue;"}
:::
::::::

## R Demonstration

Â 

**Normal Distribution**

Â 

$F(x) = P(X \leq x)$:

`pnorm(q, mean, sd, lower.tail = TRUE)`

Â 

$p^{th}$ percentile:

`qnorm(p, mean, sd, lower.tail = TRUE)`

# Sampling Distributions

## Sampling Distributions Simulation {.smaller}

Please do the following:

::: nonincremental
Answer the one question in the google form that can be accessed in any of the following ways:
:::

:::::: columns
:::: {.column width="50%"}
::: nonincremental
-   Typing the following URL into your browser. The URL is case sensitive. <https://beav.es/W4_survey>

-   Find the Week 4 Survey link on Canvas in the Quick Links module

-   Scan the QR code
:::
::::

::: {.column width="50%"}
![](QR.png){height="500"}
:::
::::::

# Key Concepts

## Inferential Statistics

::: incremental
-   Recall that inferential statistics use information from a sample to estimate or test characteristics from a population of interest.

-   Typically, we calculate a **point estimate** from the sample as our best guess of the **parameter** of interest.

    -   Naturally, our best guess for the population mean, $\mu$, from a sample is the [sample mean, $\overline{x}$.]{.fragment .fade-in}

    -   Our best guess for the population proportion, $p$, is the [sample proportion, $\hat{p}$.]{.fragment .fade-in}
:::

## Example {.smaller}

As part of a quality control process for computer chips, an engineer at a factory randomly samples 212 chips during a week of production to test the current rate of chips with severe defects. She finds that 27 of the chips are defective.

The information above describes the outcome of a single sample. Suppose the true proportion of defective chips at this factory is $p=0.1$.

:::::: fragment
::::: columns
::: {.column width="60%"}
### Participation Question ðŸ“Š

The value $p=0.1$ is the

a\. population parameter

b\. sample statistic
:::

::: {.column width="40%"}
Answer the question at

**PollEv.com/erinhowardstats**

![](pollev_qr.png){width="320"}
:::
:::::
::::::

## Sampling Variability

. . .

Even when robust sampling schemes are used, different samples will yield different point estimates.

::: fragment
![](pop.png){.absolute top="35%" right="75%" height="400"} [**Population**]{.absolute top="30%" right="83%"}
:::

::: fragment
[**Sample 1**]{.absolute top="40%" right="50%"} ![](samp1.png){.absolute top="35%" right="35%" height="100"}
:::

::: fragment
[**Sample 2**]{.absolute top="60%" right="50%"} ![](samp2.png){.absolute top="55%" right="35%" height="100"}
:::

::: fragment
[**Sample 3**]{.absolute top="80%" right="50%"} ![](samp3.png){.absolute top="75%" right="35%" height="100"}
:::

::: fragment
![](right_arrow.png){.absolute top="40%" right="25%" height="20"} [$\hat{\theta}_1$]{.absolute top="39%" right="20%"} ![](right_arrow.png){.absolute top="60%" right="25%" height="20"} [$\hat{\theta}_2$]{.absolute top="59%" right="20%"} ![](right_arrow.png){.absolute top="80%" right="25%" height="20"} [$\hat{\theta}_3$]{.absolute top="79%" right="20%"} [$\hat{\theta}$ represents a generic point estimate.]{.absolute top="95%" right="20%"}
:::

##  {.smaller}

### Distributions of Inference

::::::::: columns
:::: {.column width="32%"}
Population Distribution

::: fragment
```{r}
library(tidyverse)
library(viridis)
pool <- data.frame("times" = rgamma(10000, 2, 1.25))
ggplot(pool, aes(x = times)) +
  geom_histogram(binwidth = 0.5, color = "black",
                 boundary = 0, fill = viridis(1)) +
  labs(x = "") +
  theme(axis.line = element_line(color = "black"),
        panel.background = element_blank())
```

Distribution of the entire collection of interest.
:::
::::

:::: {.column width="32%"}
Sampl**ED** Distribution

::: fragment
```{r}
samp <- data.frame("times" = sample(pool$times, 20))
ggplot(samp, aes(x = times)) + 
  geom_histogram(binwidth = 0.5, color = "black", 
                 boundary = 0, fill = viridis(6)[6]) + 
  labs(x = "") + 
  theme(axis.line = element_line(color = "black"), 
        panel.background = element_blank())
```

Distribution of $n$ observations obtained from a single sample.
:::
::::

:::: {.column width="32%"}
Sampl**ING** Distribution

::: fragment
```{r}
library(infer)
rep_samples <- pool |>
  rep_sample_n(size = 20, reps = 10000, replace = TRUE) |>
  group_by(replicate) |>
  summarise(samp_mean = mean(times))

ggplot(rep_samples, aes(x = samp_mean)) + 
  geom_histogram(color = "black", 
                 boundary = 0, fill = viridis(6)[3]) +
  labs(x = "") + 
  theme(axis.line = element_line(color = "black"), 
        panel.background = element_blank())
```

Distribution of a sample statistic, such as $\overline{x}$ or $\hat{p}$, from repeated samples of size $n$ from the population.
:::
::::
:::::::::

## Example {.smaller}

As part of a quality control process for computer chips, an engineer at a factory randomly samples 212 chips during a week of production to test the current rate of chips with severe defects. She finds that 27 of the chips are defective.

The information above describes the outcome of a single sample. Suppose the true proportion of defective chips at this factory is $p=0.1$.

:::::: fragment
::::: columns
::: {.column width="60%"}
### Participation Question ðŸ“Š

Suppose we were able to collect every possible sample of size 212 from the population of chips produced.

The distribution of the sample proportions of defective chips is the

a\. population distribution.

b\. sampled distribution.

c\. sampling distribution.
:::

::: {.column width="40%"}
Answer the question at

**PollEv.com/erinhowardstats**

![](pollev_qr.png){width="320"}
:::
:::::
::::::

## Sampling Distributions

Â 

#### If we can't observe the sampling distribution in real-world applications, why do we care about it?

Â 

::: {.fragment style="border: 2px solid #2A788EFF; text-align: center"}
Understanding the sampling distribution of commonly used statistics, such as $\overline{x}$ and $\hat{p}$, allows us to quantify the uncertainty in our point estimates.
:::

<!-- ## Unbiased Estimators -->

<!-- Recall that because of sampling variability, a statistic from a sample is a random variable. -->

<!-- A statistic is called **unbiased** if its expectation is equal to the corresponding population parameter. -->

<!-- Â  -->

<!-- . . . -->

<!-- $\overline{x}$, $\hat{p}$, and $s^2$ are unbiased. -->

<!-- Â  -->

<!-- . . . -->

<!-- $E(\overline{x}) =$ [$\mu$]{.fragment .fade-in} -->

<!-- $E(\hat{p}) =$ [$p$]{.fragment .fade-in} -->

<!-- $E(s^2) =$ [$\sigma^2$]{.fragment .fade-in} -->

<!-- ## Consistent Estimators & The Law of Large Numbers {.smaller} -->

<!-- A point estimate is called **consistent** if it converges in probability to its corresponding population parameter. -->

<!-- Â  -->

<!-- Under the Law of Large Numbers, we have that as sample size, $n$, increases the point estimate will approach the population parameter. -->

<!-- . . . -->

<!-- Â  -->

<!-- $\overline{x}$, $\hat{p}$, and $s^2$ are consistent. -->

<!-- . . . -->

<!-- Therefore, as $n$ increases towards the size of the population -->

<!-- $\overline{x} \rightarrow$ [$\mu$]{.fragment .fade-in} -->

<!-- $\hat{p} \rightarrow$ [$p$]{.fragment .fade-in} -->

<!-- $s^2 \rightarrow$ [$\sigma^2$]{.fragment .fade-in} -->

## Sample Size & Sampling Variability {.smaller}

![](samp_size.png)

::: {incremental}
-   The variability of the point estimate is called the [standard error]{.fragment .highlight-red}.

-   The standard error is the [standard deviation]{.fragment .highlight-red} of the sampling distribution.

-   As $n$ increases, the standard error of the point estimate decreases.
:::

##  {.smaller}

### Central Limit Theorem

When observations are independent and the sample size, $n$, is sufficiently large, the central limit theorem states that the distributions of $\hat{p}$ and $\overline{x}$ are approximately Normal.

::: fragment
**The sample size conditions ("sufficiently large") and the details of these normal distributions differ for** $\hat{p}$ and $\overline{x}$.
:::

:::::::: fragment
::::::: columns
:::: {.column width="50%"}
[**Sample Proportion,** $\hat{p}$]{style="color:#440154FF"}

::: fragment
[$$\hat{p}\sim N\bigg(p, \sqrt{\frac{p(1-p)}{n}}\bigg)$$ where $p$ represents the population proportion]{style="color:#440154FF"}
:::
::::

:::: {.column width="\"50%"}
[**Sample Mean,** $\overline{x}$]{style="color:#2A788EFF"}

::: fragment
[$$\overline{x}\sim N\bigg(\mu, \frac{\sigma}{\sqrt{n}}\bigg)$$ where $\mu$ and $\sigma$ represent the population mean and standard deviation, respectively.]{style="color:#2A788EFF"}
:::
::::
:::::::
::::::::

## Central Limit Theorem

### Sampling Distribution of Sample Proportion, $\hat{p}$

```{r}
#| message: false
library(latex2exp)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "#440154FF", linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() +
  theme(axis.text.x = element_text(size = 18, color = "#440154FF", face = "bold")) + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = seq(-2,2,1), 
                     labels = c(TeX(r'(p - 2\sqrt{\frac{p(1-p)}{n}})'),
                                TeX(r'(p - \sqrt{\frac{p(1-p)}{n}})'), 
                                "p", 
                                TeX(r'(p + \sqrt{\frac{p(1-p)}{n}})'), 
                                TeX(r'(p + 2\sqrt{\frac{p(1-p)}{n}})')))
```

## Central Limit Theorem

### Sampling Distribution of Sample Mean, $\overline{x}$

```{r}
#| message: false
library(latex2exp)
library(viridis)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = viridis(6)[3], linewidth = 2) + 
  ylab("") +
  xlab("") + 
  theme_bw() +
  theme(axis.text.x = element_text(size = 18, color = viridis(6)[3], face = "bold")) + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = seq(-2,2,1), 
                     labels = c(TeX(r'(\mu - 2\frac{\sigma}{\sqrt{n}})'),
                                TeX(r'(\mu - \frac{\sigma}{\sqrt{n}})'), 
                                TeX(r'(\mu)'), 
                                TeX(r'(\mu + \frac{\sigma}{\sqrt{n}})'), 
                                TeX(r'(\mu + 2\frac{\sigma}{\sqrt{n}})')))
```

## Example {.smaller}

As part of a quality control process for computer chips, an engineer at a factory randomly samples 212 chips during a week of production to test the current rate of chips with severe defects. She finds that 27 of the chips are defective.

The information above describes the outcome of a single sample. Suppose the true proportion of defective chips at this factory is $p=0.1$.

:::::: fragment
::::: columns
::: {.column width="60%"}
### Participation Question ðŸ“Š

Suppose we were able to collect every possible sample of size 212 from the population of chips produced.

The distribution of the sample proportion of defective chips is
:::

::: {.column width="40%"}
Answer the question at

**PollEv.com/erinhowardstats**

![](pollev_qr.png){width="320"}
:::
:::::
::::::

## CLT - Sample Size Conditions {.smaller}

The sample size conditions needed to apply the Central Limit Theorem differ depending on the statistic.

::::::::: fragment
:::::::: columns
:::: {.column width="30%"}
[**Sample proportion,** $\hat{p}$]{style="color:#440154FF"}

::: fragment
[For the CLT to apply to the distribution of the sample proportion, we need the following sample size conditions to be met:]{style="color:#440154FF"}

-   [$np \geq 10$]{style="color:#440154FF"}

-   [$n(1-p) \geq 10$]{style="color:#440154FF"}
:::
::::

::: {.column width="5%"}
:::

:::: {.column width="65%"}
[**Sample mean,** $\overline{x}$]{style="color:#2A788EFF"}

::: fragment
[Use the sample size and observe the shape of the sampled distribution to determine if the sample size is sufficiently large:]{style="color:#2A788EFF"}

-   [If $n\geq 30$, we can typically assume the sampling distribution of $\overline{x}$ is approximately Normal and the CLT applies.]{style="color:#2A788EFF"}

-   [If $n < 30$, we need to look at the **sampled distribution**. If there are no clear outliers or strong skewness in the sampled data, we can assume the sampling distribution of $\overline{x}$ is approximately Normal and the CLT applies.]{style="color:#2A788EFF"}
:::
::::
::::::::
:::::::::

::: fragment
**If the sample size conditions aren't met, we cannot apply the results of the CLT.**
:::

## Example {.smaller}

As part of a quality control process for computer chips, an engineer at a factory randomly samples 212 chips during a week of production to test the current rate of chips with severe defects. She finds that 27 of the chips are defective.

The information above describes the outcome of a single sample. Suppose the true proportion of defective chips at this factory is $p=0.1$.

::::::: fragment
:::::: columns
::: {.column width="55%"}
### Participation Question ðŸ“Š

In the previous participation question, we stated that the distribution of $\hat{p}$ was Normal, but we didn't check the sample size conditions before doing so! Are the sample size conditions met in this scenario in order to apply the central limit theorem?
:::

::: {.column width="5%"}
:::

::: {.column width="40%"}
Answer the question at

**PollEv.com/erinhowardstats**

![](pollev_qr.png){width="320"}
:::
::::::
:::::::

## 

### More Practice!

Â 

Want more practice with this material? An optional activity is available. Find **Practice with the Central Limit Theorem** in the Week 4 module.
